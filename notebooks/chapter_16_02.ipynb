{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ec6540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3",
   "metadata": {},
   "source": [
    "# **Neural Cellular Automata Grafting: Jupyter Notebook**\n",
    "\n",
    "This Jupyter Notebook accompanies the chapter on **Neural Cellular\n",
    "Automata Grafting**. It includes practical examples ranging from basic\n",
    "Neural Cellular Automata (NCA) implementations to more advanced grafting\n",
    "techniques. By following along, you’ll gain hands-on experience with\n",
    "NCAs and learn how to implement grafting in Python using PyTorch.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Table of Contents**\n",
    "\n",
    "1.  [Introduction](#introduction)\n",
    "2.  [Setup and Prerequisites](#setup-and-prerequisites)\n",
    "3.  [Basic Neural Cellular Automata](#basic-neural-cellular-automata)\n",
    "    -   [Defining the NCA Model](#defining-the-nca-model)\n",
    "    -   [Training a Simple NCA](#training-a-simple-nca)\n",
    "4.  [Visualizing NCA Evolution](#visualizing-nca-evolution)\n",
    "5.  [Advanced NCA Grafting\n",
    "    Techniques](#advanced-nca-grafting-techniques)\n",
    "    -   [Training Multiple NCAs](#training-multiple-ncas)\n",
    "    -   [Spatial Grafting](#spatial-grafting)\n",
    "    -   [Parameter Grafting](#parameter-grafting)\n",
    "    -   [Dynamic Grafting](#dynamic-grafting)\n",
    "6.  [Experiments and Exercises](#experiments-and-exercises)\n",
    "7.  [Conclusion](#conclusion)\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "In this notebook, we’ll explore Neural Cellular Automata (NCA) and\n",
    "various grafting techniques to combine multiple NCAs. We’ll start with a\n",
    "basic NCA implementation and progressively move to more complex examples\n",
    "involving grafting.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Setup and Prerequisites**\n",
    "\n",
    "### **1. Install Required Libraries**\n",
    "\n",
    "Ensure you have the necessary Python libraries installed. You can\n",
    "install them using `pip`:\n",
    "\n",
    "``` python\n",
    "# Install required libraries\n",
    "!pip install torch torchvision matplotlib numpy tqdm\n",
    "```\n",
    "\n",
    "### **2. Import Libraries**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48827ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from IPython.display import HTML\n",
    "from matplotlib import animation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d076209f",
   "metadata": {},
   "source": [
    "### **3. Check for GPU Availability**\n",
    "Using a GPU can significantly speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e76b564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e82729",
   "metadata": {},
   "source": [
    "## **Basic Neural Cellular Automata**\n",
    "\n",
    "### **Defining the NCA Model**\n",
    "We’ll start by defining a simple NCA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c431f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralCA(nn.Module):\n",
    "    def __init__(self, channel_n=16, fire_rate=0.5):\n",
    "        super(NeuralCA, self).__init__()\n",
    "        self.channel_n = channel_n\n",
    "        self.fire_rate = fire_rate\n",
    "\n",
    "        # Perception layers\n",
    "        self.perception = nn.Conv2d(channel_n, channel_n * 3, kernel_size=3, padding=1, groups=channel_n, bias=False)\n",
    "        \n",
    "        # Update layers\n",
    "        self.update = nn.Sequential(\n",
    "            nn.Conv2d(channel_n * 3, 128, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, channel_n, kernel_size=1),\n",
    "        )\n",
    "        \n",
    "        # Initialize the perception kernels\n",
    "        with torch.no_grad():\n",
    "            sobel_x = torch.tensor([[1.0, 0.0, -1.0],\n",
    "                                    [2.0, 0.0, -2.0],\n",
    "                                    [1.0, 0.0, -1.0]])\n",
    "            sobel_y = torch.tensor([[1.0, 2.0, 1.0],\n",
    "                                    [0.0, 0.0, 0.0],\n",
    "                                    [-1.0, -2.0, -1.0]])\n",
    "            laplace = torch.tensor([[0.0, 1.0, 0.0],\n",
    "                                    [1.0, -4.0, 1.0],\n",
    "                                    [0.0, 1.0, 0.0]])\n",
    "            kernels = torch.stack([sobel_x, sobel_y, laplace])\n",
    "            kernels = kernels.unsqueeze(1).unsqueeze(1) / 8.0\n",
    "            self.perception.weight.data = kernels.repeat(channel_n, 1, 1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Perception\n",
    "        y = self.perception(x)\n",
    "        \n",
    "        # Update\n",
    "        dx = self.update(y)\n",
    "        \n",
    "        # Stochastic update\n",
    "        mask = (torch.rand_like(x[:, :1, :, :]) <= self.fire_rate).float()\n",
    "        dx = dx * mask\n",
    "        \n",
    "        # Apply the update\n",
    "        x = x + dx\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54303558",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "-   **Perception Layer**: Extracts local information using fixed kernels\n",
    "    (Sobel and Laplacian filters).\n",
    "-   **Update Network**: Computes state updates based on the perceived\n",
    "    information.\n",
    "-   **Stochastic Update**: Simulates asynchronous cell updates using a\n",
    "    random mask.\n",
    "\n",
    "### **Training a Simple NCA**\n",
    "\n",
    "We’ll train the NCA to generate a simple pattern, such as a circle.\n",
    "\n",
    "#### **Preparing the Target Image**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c5e9d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_target_circle(size, radius, color):\n",
    "    y, x = np.ogrid[-size/2:size/2, -size/2:size/2]\n",
    "    mask = x**2 + y**2 <= radius**2\n",
    "    img = np.zeros((size, size, 4), dtype=np.float32)\n",
    "    img[mask] = color\n",
    "    return img\n",
    "\n",
    "size = 64\n",
    "target_img = create_target_circle(size, 20, [1.0, 0.0, 0.0, 1.0])  # Red circle\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(target_img)\n",
    "plt.title(\"Target Image\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43134e58",
   "metadata": {},
   "source": [
    "#### **Defining the Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c654cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nca(model, target_img, epochs=5000, lr=2e-3, device='cpu'):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    target = torch.from_numpy(target_img).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "    losses = []\n",
    "\n",
    "    # Initialize the grid\n",
    "    x = torch.zeros(1, model.channel_n, target.shape[2], target.shape[3]).to(device)\n",
    "    x[:, :4, target.shape[2]//2, target.shape[3]//2] = 1.0  # Seed in the center\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        x.requires_grad_(True)\n",
    "        for _ in range(8):\n",
    "            x = model(x)\n",
    "            x = torch.clamp(x, 0.0, 1.0)\n",
    "        loss = F.mse_loss(x[:, :4, :, :], target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return model, x.detach(), losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574ec3cc",
   "metadata": {},
   "source": [
    "#### **Training the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc8a65ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Number of dimensions of repeat dims can not be smaller than number of dimensions of tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mNeuralCA\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      2\u001b[0m model, final_state, losses \u001b[38;5;241m=\u001b[39m train_nca(model, target_img, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "Cell \u001b[1;32mIn[16], line 30\u001b[0m, in \u001b[0;36mNeuralCA.__init__\u001b[1;34m(self, channel_n, fire_rate)\u001b[0m\n\u001b[0;32m     28\u001b[0m kernels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([sobel_x, sobel_y, laplace])\n\u001b[0;32m     29\u001b[0m kernels \u001b[38;5;241m=\u001b[39m kernels\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m8.0\u001b[39m\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperception\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43mkernels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchannel_n\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Number of dimensions of repeat dims can not be smaller than number of dimensions of tensor"
     ]
    }
   ],
   "source": [
    "model = NeuralCA().to(device)\n",
    "model, final_state, losses = train_nca(model, target_img, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dabfda5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b649210f",
   "metadata": {},
   "source": [
    "``` python\n",
    "model = NeuralCA().to(device)\n",
    "model, final_state, losses = train_nca(model, target_img, device=device)\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Visualizing NCA Evolution**\n",
    "\n",
    "We’ll visualize how the NCA evolves over time to form the target\n",
    "pattern.\n",
    "\n",
    "``` python\n",
    "def visualize_evolution(model, steps=200):\n",
    "    # Initialize the grid\n",
    "    x = torch.zeros(1, model.channel_n, size, size).to(device)\n",
    "    x[:, :4, size//2, size//2] = 1.0  # Seed in the center\n",
    "    x_history = []\n",
    "\n",
    "    for _ in range(steps):\n",
    "        x = model(x)\n",
    "        x = torch.clamp(x, 0.0, 1.0)\n",
    "        x_history.append(x[:, :4, :, :].detach().cpu().numpy())\n",
    "\n",
    "    # Create animation\n",
    "    fig, ax = plt.subplots()\n",
    "    ims = []\n",
    "\n",
    "    for img in x_history[::5]:\n",
    "        im = ax.imshow(img[0].transpose(1, 2, 0))\n",
    "        ims.append([im])\n",
    "\n",
    "    ani = animation.ArtistAnimation(fig, ims, interval=50, blit=True)\n",
    "    plt.close()\n",
    "    return HTML(ani.to_jshtml())\n",
    "\n",
    "visualize_evolution(model)\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "\n",
    "An animation showing the evolution of the NCA from the initial state to\n",
    "the final pattern.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Advanced NCA Grafting Techniques**\n",
    "\n",
    "### **Training Multiple NCAs**\n",
    "\n",
    "We’ll train a second NCA to generate a different pattern.\n",
    "\n",
    "#### **Creating a Second Target Image**\n",
    "\n",
    "``` python\n",
    "target_img2 = create_target_circle(size, 15, [0.0, 0.0, 1.0, 1.0])  # Blue circle\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(target_img2)\n",
    "plt.title(\"Second Target Image\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "![Second Target Image](attachment:second_target_image.png)\n",
    "\n",
    "#### **Training the Second Model**\n",
    "\n",
    "``` python\n",
    "model2 = NeuralCA().to(device)\n",
    "model2, final_state2, losses2 = train_nca(model2, target_img2, device=device)\n",
    "```\n",
    "\n",
    "### **Spatial Grafting**\n",
    "\n",
    "We’ll combine the two models by assigning each to different regions of\n",
    "the grid.\n",
    "\n",
    "#### **Implementing Spatial Grafting**\n",
    "\n",
    "``` python\n",
    "def graft_models(model1, model2, size, steps, device='cpu'):\n",
    "    # Initialize the grid\n",
    "    x = torch.zeros(1, model1.channel_n, size, size).to(device)\n",
    "    x[:, :4, :, :] = 0.5  # Initial state\n",
    "\n",
    "    # Create masks\n",
    "    mask1 = torch.zeros(1, 1, size, size).to(device)\n",
    "    mask1[:, :, :, :size//2] = 1.0  # Left half\n",
    "    mask2 = 1.0 - mask1  # Right half\n",
    "\n",
    "    x_history = []\n",
    "\n",
    "    for _ in range(steps):\n",
    "        x1 = model1(x)\n",
    "        x2 = model2(x)\n",
    "        x = x1 * mask1 + x2 * mask2 + x * (1 - mask1 - mask2)\n",
    "        x = torch.clamp(x, 0.0, 1.0)\n",
    "        x_history.append(x[:, :4, :, :].detach().cpu().numpy())\n",
    "\n",
    "    return x_history\n",
    "\n",
    "x_history_grafted = graft_models(model, model2, size=size, steps=200, device=device)\n",
    "```\n",
    "\n",
    "#### **Visualizing Spatial Grafting**\n",
    "\n",
    "``` python\n",
    "def visualize_grafted_evolution(x_history):\n",
    "    fig, ax = plt.subplots()\n",
    "    ims = []\n",
    "\n",
    "    for img in x_history[::5]:\n",
    "        im = ax.imshow(img[0].transpose(1, 2, 0))\n",
    "        ims.append([im])\n",
    "\n",
    "    ani = animation.ArtistAnimation(fig, ims, interval=50, blit=True)\n",
    "    plt.close()\n",
    "    return HTML(ani.to_jshtml())\n",
    "\n",
    "visualize_grafted_evolution(x_history_grafted)\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "\n",
    "An animation showing how the two NCAs interact in their respective\n",
    "regions and at the boundary.\n",
    "\n",
    "### **Parameter Grafting**\n",
    "\n",
    "We can combine the parameters of the two models.\n",
    "\n",
    "#### **Averaging Model Parameters**\n",
    "\n",
    "``` python\n",
    "def average_parameters(model1, model2):\n",
    "    model_combined = NeuralCA().to(device)\n",
    "    with torch.no_grad():\n",
    "        for p1, p2, pc in zip(model1.parameters(), model2.parameters(), model_combined.parameters()):\n",
    "            pc.copy_((p1 + p2) / 2.0)\n",
    "    return model_combined\n",
    "\n",
    "model_combined = average_parameters(model, model2)\n",
    "```\n",
    "\n",
    "#### **Visualizing Parameter Grafting**\n",
    "\n",
    "``` python\n",
    "def visualize_combined_model(model_combined, steps=200):\n",
    "    x = torch.zeros(1, model_combined.channel_n, size, size).to(device)\n",
    "    x[:, :4, size//2, size//2] = 1.0  # Seed in the center\n",
    "    x_history = []\n",
    "\n",
    "    for _ in range(steps):\n",
    "        x = model_combined(x)\n",
    "        x = torch.clamp(x, 0.0, 1.0)\n",
    "        x_history.append(x[:, :4, :, :].detach().cpu().numpy())\n",
    "\n",
    "    # Create animation\n",
    "    fig, ax = plt.subplots()\n",
    "    ims = []\n",
    "\n",
    "    for img in x_history[::5]:\n",
    "        im = ax.imshow(img[0].transpose(1, 2, 0))\n",
    "        ims.append([im])\n",
    "\n",
    "    ani = animation.ArtistAnimation(fig, ims, interval=50, blit=True)\n",
    "    plt.close()\n",
    "    return HTML(ani.to_jshtml())\n",
    "\n",
    "visualize_combined_model(model_combined)\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "\n",
    "An animation showing the behavior of the combined model with averaged\n",
    "parameters.\n",
    "\n",
    "### **Dynamic Grafting**\n",
    "\n",
    "Switch between models during simulation.\n",
    "\n",
    "#### **Implementing Dynamic Grafting**\n",
    "\n",
    "``` python\n",
    "def dynamic_grafting(model1, model2, size, steps, switch_step, device='cpu'):\n",
    "    x = torch.zeros(1, model1.channel_n, size, size).to(device)\n",
    "    x[:, :4, size//2, size//2] = 1.0  # Seed in the center\n",
    "    x_history = []\n",
    "\n",
    "    for step in range(steps):\n",
    "        if step < switch_step:\n",
    "            x = model1(x)\n",
    "        else:\n",
    "            x = model2(x)\n",
    "        x = torch.clamp(x, 0.0, 1.0)\n",
    "        x_history.append(x[:, :4, :, :].detach().cpu().numpy())\n",
    "\n",
    "    return x_history\n",
    "\n",
    "x_history_dynamic = dynamic_grafting(model, model2, size=size, steps=200, switch_step=100, device=device)\n",
    "```\n",
    "\n",
    "#### **Visualizing Dynamic Grafting**\n",
    "\n",
    "``` python\n",
    "visualize_grafted_evolution(x_history_dynamic)\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "\n",
    "An animation showing how the NCA’s behavior changes when switching from\n",
    "one model to another during the simulation.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Experiments and Exercises**\n",
    "\n",
    "1.  **Modify the Target Images**: Try using different shapes or patterns\n",
    "    as target images, such as squares, letters, or more complex\n",
    "    drawings.\n",
    "\n",
    "2.  **Adjust Fire Rates**: Experiment with different `fire_rate` values\n",
    "    in the `NeuralCA` model (e.g., 0.1, 0.9) and observe how it affects\n",
    "    the evolution.\n",
    "\n",
    "3.  **Create Gradient Masks**: Instead of binary masks, use masks with\n",
    "    gradient values to blend updates smoothly between models in spatial\n",
    "    grafting.\n",
    "\n",
    "    ``` python\n",
    "    # Create a gradient mask\n",
    "    mask1 = torch.linspace(1.0, 0.0, steps=size).unsqueeze(0).unsqueeze(0).unsqueeze(3).repeat(1, 1, 1, size).to(device)\n",
    "    mask2 = 1.0 - mask1\n",
    "    ```\n",
    "\n",
    "4.  **Train Combined Models**: After grafting models (especially\n",
    "    parameter grafting), continue training the combined model and see if\n",
    "    it can learn new patterns or improve existing ones.\n",
    "\n",
    "5.  **Extend to 3D NCAs**: Modify the code to work with 3D grids and\n",
    "    visualize the results using volumetric rendering techniques.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "In this notebook, we’ve explored Neural Cellular Automata from basic\n",
    "implementations to advanced grafting techniques. By combining multiple\n",
    "NCAs, we can create complex and dynamic patterns that showcase the power\n",
    "of local interactions leading to emergent global behaviors.\n",
    "\n",
    "Feel free to experiment further, modify the code, and explore new ideas\n",
    "in the fascinating world of NCAs.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "**Note**: Make sure to run all the code cells in order to avoid any\n",
    "errors due to undefined variables or missing dependencies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
