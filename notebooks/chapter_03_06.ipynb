{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Emergent Dynamics in Neural Cellular Automata](https://arxiv.org/pdf/2404.06406)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import DTD\n",
    "from torchvision import transforms\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Convolution Kernels\n",
    "The paper uses Sobel filters for x and y directions, as well as a Laplacian filter, and an identity filter in the convolution. \n",
    "\n",
    "We define them as PyTorch tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_kernels():\n",
    "  \"\"\"Defines the convolution kernels used in the NCA.\"\"\"\n",
    "  kx = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32)\n",
    "  ky = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32)\n",
    "  klap = torch.tensor([[1, 1, 1], [1, -8, 1], [1, 1, 1]], dtype=torch.float32)\n",
    "  kid = torch.tensor([[0, 0, 0], [0, 1, 0], [0, 0, 0]], dtype = torch.float32) # Identity filter\n",
    "  return kx, ky, klap, kid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the NCA Module\n",
    "\n",
    "Create a custom PyTorch nn.Module to encapsulate the NCA logic, making it easier to integrate into the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCA(nn.Module):\n",
    "    def __init__(self, num_channels, hidden_neurons):\n",
    "        super(NCA, self).__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.hidden_neurons = hidden_neurons\n",
    "\n",
    "        kx, ky, klap, kid = get_conv_kernels()\n",
    "        self.conv_kernels = torch.stack([kx, ky, klap, kid]).unsqueeze(1)\n",
    "\n",
    "        # Define the MLP for the update rule\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(4 * num_channels, hidden_neurons),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_neurons, num_channels)\n",
    "        )\n",
    "\n",
    "\n",
    "    def perception(self, state):\n",
    "        \"\"\"Applies the convolution kernels to the cell state.\"\"\"\n",
    "        b, c, h, w = state.shape\n",
    "        # Move filters to the device\n",
    "        conv_kernels = self.conv_kernels.to(state.device)\n",
    "        # Perform convolution\n",
    "        # Note: We need to pad the edges of the state in the correct manner \n",
    "        padded_state = F.pad(state, (1,1,1,1), mode='circular')\n",
    "        # Apply depthwise convolution\n",
    "        conv_out = F.conv2d(padded_state, conv_kernels, padding=0, groups=c)\n",
    "        # Reshape conv_out to combine all channels to run through MLP\n",
    "        conv_out = conv_out.permute(0, 2, 3, 1).reshape(b, h, w, -1)\n",
    "        return conv_out\n",
    "\n",
    "    def update(self, state):\n",
    "       \"\"\"Applies one update rule of the NCA.\"\"\"\n",
    "       b, c, h, w = state.shape\n",
    "\n",
    "       # Compute Perception step using convolution\n",
    "       perception_output = self.perception(state)\n",
    "\n",
    "       # Apply MLP to get output\n",
    "       mlp_output = self.mlp(perception_output)\n",
    "\n",
    "\n",
    "       # Generate a random binary mask to achieve asynchronicity\n",
    "       mask = (torch.rand(b, h, w, 1, device=state.device) > 0.5).float()\n",
    "\n",
    "       # Apply the stochastic mask and update the state\n",
    "       state = state + (mlp_output.permute(0, 3, 1, 2) * mask)\n",
    "       return state\n",
    "\n",
    "    def forward(self, initial_state, steps):\n",
    "        \"\"\"Applies the update rule for a specified number of steps.\"\"\"\n",
    "        state = initial_state\n",
    "        for _ in range(steps):\n",
    "            state = self.update(state)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "def download_file(url, destination):\n",
    "    \"\"\"Downloads a file to a local directory.\"\"\"\n",
    "    if not os.path.exists(destination):\n",
    "        print(f\"Downloading {url} to {destination}\")\n",
    "        urlretrieve(url, destination)\n",
    "\n",
    "def download_spynet_weights():\n",
    "     \"\"\"Downloads the pretrained weights for the optical flow estimation.\"\"\"\n",
    "     spynet_url = \"https://github.com/sniklaus/pytorch-spynet/releases/download/v1.0/spynet-sintel-final.pytorch\"\n",
    "     destination_dir = \"./\"\n",
    "     destination_filename = os.path.join(destination_dir, \"spynet-sintel-final.pytorch\")\n",
    "     \n",
    "     download_file(spynet_url, destination_filename)\n",
    "     return destination_filename\n",
    "\n",
    "def compute_optical_flow(image1, image2, weights_path):\n",
    "    \"\"\"Computes the optical flow between two images.\"\"\"\n",
    "    try:\n",
    "        from optical_flow import SpyNet\n",
    "    except:\n",
    "       print(\"Please move spynet.py file into the root of the project, refer to https://github.com/pytorch/examples/tree/main/optical_flow for reference\")\n",
    "       return None\n",
    "    spynet = SpyNet().cuda()\n",
    "\n",
    "    # Load the weights\n",
    "    state_dict = torch.load(weights_path)\n",
    "    spynet.load_state_dict(state_dict)\n",
    "\n",
    "    image1 = image1.unsqueeze(0).cuda()\n",
    "    image2 = image2.unsqueeze(0).cuda()\n",
    "    flow = spynet(image1, image2).detach().cpu()\n",
    "    return flow\n",
    " \n",
    "def motion_strength(flow):\n",
    "    \"\"\"Calculates the motion strength of the given optical flow.\"\"\"\n",
    "    norm = torch.sqrt(flow[:,0,:,:]**2+flow[:,1,:,:]**2)\n",
    "    return torch.mean(norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nca(nca_model, dataset, epochs=6000, learning_rate=1e-3, batch_size = 1, steps_per_frame = 32, frames_per_metric = 100):\n",
    "   optimizer = Adam(nca_model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "   motion_strengths = []\n",
    "   for epoch in range(epochs):\n",
    "        for i, data in enumerate(dataset):\n",
    "             optimizer.zero_grad()\n",
    "             image = data[0].to(device)\n",
    "             b, c, h, w = image.shape\n",
    "             initial_state = torch.randn(b,nca_model.num_channels,h,w, device=device)\n",
    "             \n",
    "             final_state = nca_model(initial_state, steps_per_frame * frames_per_metric)\n",
    "             \n",
    "             # Motion strength is calculated using frames_per_metric sequential frames\n",
    "             avg_motion_strength = 0\n",
    "             for frame_idx in range(frames_per_metric - 1):\n",
    "                img1 = final_state[:, :3, :, :].clone()\n",
    "                img2 = nca_model(initial_state, (frame_idx + 1) * steps_per_frame)[:, :3, :, :].clone()\n",
    "                \n",
    "                # Normalize between 0 and 1 to be useable in the optical flow computation\n",
    "                img1 = (img1 - img1.min()) / (img1.max() - img1.min())\n",
    "                img2 = (img2 - img2.min()) / (img2.max() - img2.min())\n",
    "                \n",
    "                flow = compute_optical_flow(img1[0], img2[0], weights_path)\n",
    "                if flow is not None:\n",
    "                 avg_motion_strength += motion_strength(flow)\n",
    "\n",
    "             avg_motion_strength /= (frames_per_metric - 1)\n",
    "\n",
    "             loss = -avg_motion_strength\n",
    "             loss.backward()\n",
    "             optimizer.step()\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "         print(f\"Epoch: {epoch} Loss: {loss.item()}, avg_motion_strength: {avg_motion_strength.item()}\")\n",
    "        motion_strengths.append(avg_motion_strength.item())\n",
    "   return motion_strengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dtd: Dataset DTD\n",
      "    Number of datapoints: 1880\n",
      "    Root location: ./data\n",
      "    split=train, partition=1\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=True)\n",
      "           )\n",
      "Content of 'labels_joint_anno.txt'\n",
      "Content of 'test1.txt'\n",
      "Content of 'test10.txt'\n",
      "Content of 'test2.txt'\n",
      "Content of 'test3.txt'\n",
      "Content of 'test4.txt'\n",
      "Content of 'test5.txt'\n",
      "Content of 'test6.txt'\n",
      "Content of 'test7.txt'\n",
      "Content of 'test8.txt'\n",
      "Content of 'test9.txt'\n",
      "Content of 'train1.txt'\n",
      "Content of 'train10.txt'\n",
      "Content of 'train2.txt'\n",
      "Content of 'train3.txt'\n",
      "Content of 'train4.txt'\n",
      "Content of 'train5.txt'\n",
      "Content of 'train6.txt'\n",
      "Content of 'train7.txt'\n",
      "Content of 'train8.txt'\n",
      "Content of 'train9.txt'\n",
      "Content of 'val1.txt'\n",
      "Content of 'val10.txt'\n",
      "Content of 'val2.txt'\n",
      "Content of 'val3.txt'\n",
      "Content of 'val4.txt'\n",
      "Content of 'val5.txt'\n",
      "Content of 'val6.txt'\n",
      "Content of 'val7.txt'\n",
      "Content of 'val8.txt'\n",
      "Content of 'val9.txt'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DTD' object has no attribute 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 32\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/dtd/dtd/labels\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m dtd_indices \u001b[38;5;241m=\u001b[39m [idx \u001b[38;5;28;01mfor\u001b[39;00m idx, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mdtd_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabels\u001b[49m) \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;241m==\u001b[39m texture_names[texture_idx]]\n\u001b[0;32m     33\u001b[0m subset_dataset \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mSubset(dtd_dataset, dtd_indices)\n\u001b[0;32m     34\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(subset_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DTD' object has no attribute 'labels'"
     ]
    }
   ],
   "source": [
    "# # Download spynet weights\n",
    "# weights_path = download_spynet_weights()\n",
    "\n",
    "# Download the DTD dataset, and select one of the four textures as in the paper\n",
    "# We are downloading this to './data'\n",
    "dataset_path = './data'\n",
    "\n",
    "if not os.path.isdir(dataset_path):\n",
    "   os.makedirs(dataset_path)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((128, 128))])\n",
    "\n",
    "\n",
    "dtd_dataset = DTD(root=dataset_path, split='train', download=True, transform=transform)\n",
    "\n",
    "dataset_dir = os.path.join(dataset_path, 'dtd')\n",
    "labels_dir = os.path.join(dataset_dir, 'labels')   \n",
    "texture_idx = 0\n",
    "texture_names = ['bubbly_0101', 'chequered_0121', 'interlaced_0172', 'cracked_0085']\n",
    "print(f\"Dtd: {dtd_dataset}\")\n",
    "\n",
    "# Import module\n",
    "import os\n",
    "\n",
    "# Assign directory\n",
    "directory = r\"gfg-test\"\n",
    "\n",
    "# Iterate over files in directory\n",
    "for name in os.listdir(\"./data/dtd/dtd/labels\"):\n",
    "    print(f\"Content of '{name}'\")\n",
    "\n",
    "\n",
    "dtd_indices = [idx for idx, label in enumerate(dtd_dataset.labels) if label == texture_names[texture_idx]]\n",
    "subset_dataset = torch.utils.data.Subset(dtd_dataset, dtd_indices)\n",
    "dataloader = torch.utils.data.DataLoader(subset_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "   \n",
    "C_values = range(8,128+1, 8)\n",
    "D_values = range(16, 128+1, 16)\n",
    "\n",
    "results = {}\n",
    "# Loop through the values\n",
    "for C in C_values:\n",
    "    for D in D_values:\n",
    "        print(f\"Training for C: {C}, D: {D}\")\n",
    "        nca_model = NCA(num_channels=C, hidden_neurons=D).to(device)\n",
    "        motion_strengths = train_nca(nca_model, dataloader)\n",
    "        results[(C,D)] = motion_strengths\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "for C,D in results.keys():\n",
    "    plt.plot(results[(C,D)], label = f\"C: {C}, D: {D}\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Motion Strength\")\n",
    "plt.title(\"Motion Strength vs Training Epochs for various C & D\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# Now, you can use the trained model for visualization and experimentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
